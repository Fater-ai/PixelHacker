# cog.yaml
build:
  gpu: true
  python_version: "3.10"  # As suggested by PixelHacker's conda setup
  cuda: "12.1"            # Compatible with PyTorch 2.3.0

  # System packages can be installed here if needed, e.g., build-essentials for some packages
  # system_packages:
  #   - "build-essential"

  python_packages:
    - "accelerate==0.34.0"
    - "diffusers==0.30.2"
    - "einops==0.7.0"
    - "fla==0.1"
    # flash-attn will be installed via a 'run' command below
    - "lightning==2.5.1"
    - "lpips==0.1.4"
    - "numpy==1.24.4"
    - "omegaconf==2.3.0"
    - "opencv-python==4.8.0.74"
    - "protobuf==3.19.0"
    - "PyWavelets==1.6.0"
    - "PyYAML==6.0.1"
    - "safetensors==0.4.3"
    - "scikit-learn==1.2.0"
    - "scipy==1.12.0"
    - "six==1.16.0"
    - "sympy==1.12"
    - "tensorboard==2.9.0"
    - "timm==0.9.11"
    - "tokenizers==0.19.0"
    - "toml==0.10.2"
    - "torch==2.3.0"
    - "torchinfo==1.8.0"
    - "torchmetrics==1.6.0"
    - "torchvision"
    - "tqdm==4.66.2"
    - "transformers==4.40.0"
    # triton==3.2.0 # Triton often has complex build dependencies.
                     # PyTorch 2.3.0 might come with a compatible Triton or it might build from source.
                     # If issues arise, we might need to be more specific or pre-build it.
                     # For now, let's see if it installs correctly. If not, try removing it
                     # and see if flash-attn still works, or install it via a run command.
    - "huggingface-hub"
    - "Pillow"

  run:
    # Install flash-attn with the required environment variable and options
    - "FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn==2.5.8 --no-build-isolation"
    # Triton can also be problematic, if the direct pip install above fails, try installing it here too.
    # It's generally best if torch can pull its own compatible triton.
    # - "pip install triton==3.2.0" # Only if the python_packages entry for triton fails

predict: "predict.py:Predictor"